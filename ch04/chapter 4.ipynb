{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Word2Vec 속도 개선 \n",
    "\n",
    "앞장과 비교해서 두가지 개선을 추가할 것입니다. 첫 번째 개선으로는 Embedding이라는 새로운 계층을 도입하고 두 번째로는 네거티브 샘플링을 통해 새로운 손실 함수를 도입합니다. \n",
    "앞에서 얘기했던 CBOW 모델을 큰 말풍선에 활용을 할 경우에 중간 계산에 많은 시간이 소요됩니다. 원핫 표현과 가중치 행렬의 곱 계산/은닉층과 가중치 행렬의 곱 및 Softmax 계층의 계산이 \n",
    "시간을 많이 소요하게 만드는 요인입니다. \n",
    "\n",
    "Embedding 계층 \n",
    "\n",
    "자연어 처리 분야에서 단어의 밀집벡터 표현을 단어 임베딩 혹은 단어의 분산 표현이라 합니다. 행렬에서 특정 행을 추출하기란 아주 쉽습니다. 가중치 W가 2차원 넘파이 배열일 때, \n",
    "이 가중치로부터 특정 행을 추출하려면 그저 W[2]나 W[5]처럼 원하는 행을 명시하면 끝입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]]\n",
      "[6 7 8]\n",
      "[15 16 17]\n"
     ]
    }
   ],
   "source": [
    "## 추출 \n",
    "import numpy as np \n",
    "W = np.arange(21).reshape(7, 3)\n",
    "print(W)\n",
    "print(W[2])\n",
    "print(W[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 임베딩 forward method 구현 \n",
    "\n",
    "class Embedding: \n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    ## 역전파에서는 반대로 gradient값이다\n",
    "    ## 안좋은예 \n",
    "    # def backward(self, dout):\n",
    "    #     dw, = self.grads\n",
    "    #     dw[...] = 0\n",
    "    #     dw[self.idx] = dout\n",
    "    #     return None \n",
    "    \n",
    "    ## 이 코드는 가중치 기울기 dW를 꺼낸 다음, dw의 원소를 0으로 덮어씁니다. 그리고 앞층에서 전해진 기울기 dout을 idx번째 행에 할당합니다. \n",
    "    ## 문제 \n",
    "    ## idx의 원소가 중복될 때 발생합니다. \n",
    "    ## 그래서 할당이 아닌 더하기를 해야합니다. \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        if GPU:\n",
    "            np.scatter_add(dW, self.idx, dout)\n",
    "        else:\n",
    "            np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "남은 병목은 은닉층 이후의 처리입니다. 이 병목을 해소하기 위해 네거티브 샘플링이라는 기법을 사용해서 말이죠. \n",
    "이 기법은 다중 분류를 이진 부류로 근사하는 것으로 이해하는 게 중점입니다. \n",
    "\n",
    "은닉층에서는 특정 단어에 대한 열벡터만을 가지고 와서 계산합니다.\n",
    "\n",
    "이전까지의 출력층에서는 모든 단어를 대상으로 계산을 수행했습니다. 하지만 여기서는 특정 단어 하나에 주목하여 그 점수만을 계산합니다. \n",
    "그리고 시그모이드 함수를 이용해 그 점수를 확률로 변환합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ch04'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\edd6d\\Deep Learning From Scratch2\\Chapter 4\\chapter 4.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/edd6d/Deep%20Learning%20From%20Scratch2/Chapter%204/chapter%204.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/edd6d/Deep%20Learning%20From%20Scratch2/Chapter%204/chapter%204.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/edd6d/Deep%20Learning%20From%20Scratch2/Chapter%204/chapter%204.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcbow\u001b[39;00m \u001b[39mimport\u001b[39;00m CBOW\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/edd6d/Deep%20Learning%20From%20Scratch2/Chapter%204/chapter%204.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskip_gram\u001b[39;00m \u001b[39mimport\u001b[39;00m SkipGram\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/edd6d/Deep%20Learning%20From%20Scratch2/Chapter%204/chapter%204.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m create_contexts_target, to_cpu, to_gpu\n",
      "File \u001b[1;32mc:\\Users\\edd6d\\Deep Learning From Scratch2\\Chapter 4\\cbow.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnp\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# import numpy as np\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Embedding\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mch04\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnegative_sampling_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m NegativeSamplingLoss\n\u001b[0;32m      9\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCBOW\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, vocab_size, hidden_size, window_size, corpus):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ch04'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common import config\n",
    "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
    "# ===============================================\n",
    "# config.GPU = True\n",
    "# ===============================================\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from cbow import CBOW\n",
    "from skip_gram import SkipGram\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if config.GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "\n",
    "# 모델 등 생성\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
    "word_vecs = model.word_vecs\n",
    "if config.GPU:\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
